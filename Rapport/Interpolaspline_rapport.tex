\documentclass[a4paper,12pt]{article} % Changer la taille de police c'est ici

\usepackage{framed} % Marges
\usepackage[utf8]{inputenc} %francais
\usepackage[T1]{fontenc} %francais
\usepackage[french]{babel}  %francais
\usepackage{lmodern} % Pour changer le pack de police
\usepackage{makeidx} % Index
\usepackage{graphicx} % Figures
\usepackage{wrapfig} % Figures
\usepackage{amsmath} % Maths
\usepackage{amssymb} % symboles ?
\usepackage{bclogo} % ?????
\usepackage{hyperref} % URL
\usepackage{stmaryrd}
\usepackage{titling} %Mise en page du titre
\usepackage[top=2cm, bottom=2cm, left=2cm, right=2cm]{geometry} %Marges
\usepackage{float} %Pour image (H)
\usepackage{subcaption}

% numérotation et mise en titre des paragraphes et subparagraphes
\setcounter{secnumdepth}{6}
\renewcommand\theparagraph{\Alph{paragraph}}
     
\makeatletter
\renewcommand\paragraph{\@startsection{paragraph}{4}{\z@}%
                                      {-3.25ex\@plus -1ex \@minus -.2ex}%
                                      {0.0001pt \@plus .2ex}%
                                      {\normalfont\normalsize\bfseries}}
\renewcommand\subparagraph{\@startsection{subparagraph}{5}{\z@}%
                                      {-3.25ex\@plus -1ex \@minus -.2ex}%
                                      {0.0001pt \@plus .2ex}%
                                      {\normalfont\normalsize\bfseries}}
     
\counterwithin{paragraph}{subsubsection}
\counterwithin{subparagraph}{paragraph}

%reset des numéros des subsection au changement de partie
\csname @addtoreset\endcsname{section}{part} 

% Descend le titre

\makeatother


\setlength{\droptitle}{3cm}

\title{\textbf{Interpolaspline}\\ rapport}
\author{CORBILLE Clément, DOUMBOUYA Mohamed, EL BOUCHOUARI Zakaria, \\HEDDIA Bilel, PIASENTIN Béryl, RODET Amélys }
\date{Avril 2020}

\begin{document}

\maketitle

\newpage
\tableofcontents

\renewcommand\partname{}
\part{Introduction}
    Dans le cadre de notre troisième année de licence en mathématiques et informatique, un stage applicatif est obligatoire. L'objectif de ce stage est de nous préparer au milieu professionnel. Il nous initie au projet de groupe et à la relation avec Mr Perrier, chercheur à l'INRIA et client de notre travail durant quatre semaines. 

    Le projet consiste en la création d'une fonction interpolant des données ainsi qu'en la minimisation de l'impact de données dites "aberrantes" sur cette fonction. Nos objectifs sont donc de trouver la fonction de manière automatique, et de chercher plusieurs méthodes pour détecter les données aberrantes et les traiter.\\

    Dans ce compte rendu, nous allons en premier vous décrire le projet, en particulier le sujet et l'organisation mise en place durant les trois semaines de réalisation. En seconde partie, nous vous présenterons les définitions sur lesquelles nous nous sommes basés, les différentes méthodes découvertes suite à nos recherches, mais aussi une comparaison des résultats produits par nos différents algorithmes. Nous expliquerons et illustrerons ensuite le fonctionnement de notre application, qui répond aux besoins du client, avant de conclure ce projet.

\renewcommand\partname{}
\part{Description du projet}
	\section{Sujet}
	    En raison de l'utilisation intensive des données, la gestion des valeurs aberrantes a acquis une grande importance ces dernières années. La présence de valeurs aberrantes peut alors conduire à de faux résultats, ce qui peut inciter des décisions risquées.
		C'est pourquoi nous avons cherché durant ce projet des solutions susceptibles de gérer les données aberrantes.\\
		
        L'objectif principal de ce projet est de chercher des méthodes qui gèrent ces points aberrants, lors d'une interpolation des données.
		\subsection{Splines}
		    Nous allons, pour étudier la gestion des valeurs aberrantes, générer des valeurs et essayer de les interpoler ou de les approximer par une fonction (possédant certaines propriétés, comme la continuité). Cette fonction sera une spline, nous définirons ce terme plus loin dans le rapport.\\
		    
			 La différence entre l'interpolation et l'approximation mérite d'être expliquée : l'interpolation est la construction d'une fonction qui passe par toutes nos données, tandis que la fonction construite par approximation ne passera pas forcement par tous nos points mais fera au mieux (cette seconde option est privilégiée en présence d'un nuage de points). Les figures suivantes illustrent la différence entre interpolation et approximation.
			 
			  \begin{figure}[h]
                    \centering
                    \includegraphics[width=10cm]{Images/interpolation.png}
                    \caption{Exemple d'interpolation sur un jeu de données quelconque}
                    \includegraphics[width=10cm]{Images/approximation.png}
                    \caption{Exemple d'approximation avec mêmes données}
                \end{figure}
			 
			 Cependant, l'interpolation et l'approximation ont un objectif commun : créer une fonction qui correspond le mieux possible aux données. C'est pourquoi nous allons nous permettre un abus de langage dans la suite de ce rapport : le terme interpolation sera le seul utilisé, même dans le cas d'approximation (pour les splines de lissage par exemple).
			 
		\subsection{Données aberrantes}
		    Nous avons parlé jusque là de données aberrantes sans les définir réellement. Nous allons ici discuter des différents sens existants, et expliciter la définition considérée pour le reste du rapport.\\
		    
		    Les valeurs aberrantes peuvent être des données qui n'ont aucun sens comme par exemple une réponse absurde dans un questionnaire ou bien une erreur de mesure. On souhaite les ignorer totalement. 
		    
		    Les données aberrantes peuvent aussi être définies comme des valeurs qui se situent loin de la majorité des autres valeurs mais qui sont nécessaires pour l'étude, comme par exemple l'âge des étudiants du supérieur : une grande majorité aura un âge inférieur à 30 ans, mais il peut y en avoir quelques-uns ayant plus de 60 ans (même si cela est très rare) : nous voulons aussi intégrer ces étudiants dans les analyses. Ils constituent des données aberrantes, mais ce n'est pas une réponse absurde ou une erreur de mesure.
		    
		    La définition que l'on retient pour la suite du rapport va être moins détaillées que les deux proposées précédemment, car nous générons des données aberrantes, mais nous n'en connaissons pas l'origine (Personne particulière ? Erreur de saisie ?). Pour nous, une donnée aberrante est donc définie comme une donnée distante des autres observations effectuées sur le même phénomène.
		    
		    
	\section{Organisation}
	    L'organisation de ces trois semaines de stage applicatif a été planifiée lors de la première semaine du projet, en décembre, qui était plus amplement dédiée à la gestion de projet.
        Lors de cette semaine, nous avions conclu que Béryl serait notre chef de projet. Elle a veillé au mieux sur le bon déroulement de chaque tâche, et a facilité la communication au sein de l'équipe en plus de sa participation à certaines tâches.
    
        Le premier jour a permis à tout le monde de se remettre dans le sujet et de se plonger dans leurs tâches respectives. Pour la suite, ces trois semaines de développement et de recherche ont permis à chaque membre du groupe d'être responsable du bon fonctionnement d'une ou de plusieurs tâches. Notre approche avant ces trois semaines était que les tâches devaient se terminer dans les délais, fixés par nous-mêmes.
        Mais nous nous sommes logiquement rendus compte qu'avec notre faible expérience dans l'organisation de travaux de groupes et les nombreux facteurs (humains et matériels) qui interviennent, la planification allait être remaniée maintes et maintes fois (nous pouvons voir entre les annexes num et num la différence entre la répartition des tâches originelle et finale).
        
        Nous avons donc rapidement remarqué que la planification que nous avions faite de ce projet ne serait pas respectée. Mais grâce à notre communication, nous avons pu limiter les problèmes de dépendances des tâches et avancer correctement, de manière à effectuer le plus grand nombre possible de nos tâches avant la fin de ces trois semaines.\\
        
        Notre communication textuelle et orale en temps de confinement s'est principalement basée sur le logiciel Discord. Sa facilité d'utilisation nous a permis de tenir régulièrement informé le groupe sur nos avancées respectives, et de faire quelques petites réunions entre nous. Ce logiciel nous a également servi à contacter le client lors de questions à propos de ses attentes précises.
        
        Le reste de la communication au sein de l'équipe s'est quant à elle basée sur plusieurs logiciels, comme Git (GitKraken, Github et Git) pour les fichiers d'implémentation, Overleaf pour le rapport, et Google Documents pour les documents d'organisation.
        
	\section{Éléments fournis par le client}
	    Ce projet est un projet de recherche. Cependant, l'objectif ne se limite pas au traitement de données aberrantes : nous devons nous baser sur les besoins du client car il est le premier concerné par ce projet. Mr Perrier nous a fourni un générateur de signaux bruités : notre objectif est de réussir à reconstruire ces signaux malgré le bruit. \\
	    
	    Le programme fourni est en python. Il possède plusieurs fonctions. Ce programme nous permet de générer un ensemble de points en discrétisant un signal (stationnaire  ou non) et en lui appliquant ensuite un bruit afin de faire apparaître des points aberrants.
	    
	    Les signaux stationnaires sont des courbes lisses tandis que les signaux non stationnaires sont crénelés. 



\renewcommand\partname{}
\part{Réalisation}
	Pour interpoler les données, il faut choisir un modèle d'interpolation. Nous allons en évoquer quelques-uns. 
	
    Chaque modèle nécessite d'estimer les valeurs prises par une fonction continue entre deux points déterminés. La fonction et ses propriétés dépendent du modèle choisi. 
	
	L'interpolation linaire est considérée comme le modèle le plus simple. La fonction recherchée est une droite, dont l'équation est donc de la forme $f(x)=a*x+b$ avec $a$ et $b$ des paramètres réels. L'interpolation ne peut être faite qu'avec des points alignés. Si ce n'est pas le cas, il faut faire de l'approximation. Cette méthode est facile et simple à utiliser grâce au faible nombre de paramètres, mais sa précision dépend beaucoup des données : si elles sont très dispersées et loin d'un alignement, la droite ne sera pas très représentative des données.
	
    Un second type d'interpolation assez connu est l'interpolation polynomiale. L'interpolation polynomiale consiste à interpoler les données par un polynôme d'un degré fixé. Le polynôme interpolant précisément des données est unique. L'interpolation de Lagrange est l'une des méthodes les plus célèbres pour l'interpolation polynomiale : c'est une version simple, qui impose simplement le passage du polynôme par tous les points donnés $(x_i,y_i)$. Étant donné un ensemble de n+1 points  on cherche à trouver un polynôme $P$ de degré n au plus qui vérifie $P(xi)=y_i  ,i=0...n$. Malheureusement, le résultat n'est pas toujours à la hauteur des espérances car lorsqu'il s'agit d'un modèle avec plusieurs points à interpoler, les polynômes obtenus vont avoir de très grands degrés : cela engendre des oscillations. Cette problématique nous a poussés à chercher d'autres solutions. 

	\section{Splines}
	    Soit $n$ le nombre de données à interpoler.
	    Une spline est une fonction définie par morceaux. Les points délimitant les morceaux sont appelés les noeuds de la spline. Chaque morceau est un polynôme. Il y aurait, avec cette définition, une infinité de fonctions possible pour un seul jeu de données. C'est pourquoi il y a une contrainte supplémentaire : la fonction globale doit être $C^k$, avec $k\in\mathbb{N}$. Cela signifie que la fonction qui interpole les données doit être continue, et toutes ses dérivées jusqu'à la $k^{ième}$ inclue doivent l'être également.\\
	    
	    L'interpolation par une spline constitue donc une alternative à l'interpolation par un polynôme de haut degré car les polynômes de la splines peuvent être de bas degré.\\
		
		\subsection{Splines cubiques $C^2$ naturelles}

            Les splines cubiques sont composées de polynômes de degré trois. Nous allons uniquement considérer les splines $C^2$ dans ce projet. \\
            
            Soit $m$ le nombre de noeuds de la spline, ici confondus avec les données que l'on souhaite interpoler. Ils délimitent $m-1$ intervalles, et donc $m-1$ polynômes. Chaque polynôme de degré trois possède 4 inconnues, ce qui donne $4(m-1) = 4m - 4$ inconnues. Étudions maintenant les contraintes : 
            \begin{itemize}
                \item[•] $C^0 \Rightarrow 2m-2$ contraintes.\\ 
                En effet, les noeuds possèdent chacun une valeur. Chaque polynôme passe par 2 noeuds, donc possède deux contraintes : au total, la condition $C^0$ engendre $(m-1)*2 = 2m-2$ contraintes. 
                \item[•] Dérivée première continue $\Rightarrow m-2$ contraintes. \\
                En effet, chaque noeud interne (on exclue le premier et le dernier noeud) doit avoir la même dérivée à droite et à gauche, ce qui donne une contrainte (la valeur de la dérivée) par noeud. Il y a $m-2$ noeuds internes, donc la continuité de la dérivée première engendre $m-2$ contraintes.
                \item[•] Dérivée seconde continue $\Rightarrow m-2$ contraintes. \\
                Le raisonnement est identique à celui de la dérivée première.
            \end{itemize}
            On obtient au total $(2m-2) + (m-2) + (m-2) = 4m-6$ contraintes. Cela nous donne $(4m-4) - (4m-6) = 2$ degrés de liberté, ce qui implique une infinité de fonctions solutions.\\
            
            Pour avoir une unique spline cubique $C^2$ qui interpole les $m$ noeuds associés à un jeu de données, il ne faut plus avoir de degré de liberté. Une possibilité est de décréter les dérivées premières aux extrémités nulles : cela ajoute deux contraintes, ce qui enlève les deux degrés de liberté laissés par la définition des splines cubiques $C^2$. Lorsque c'est cette solution qui est choisie, la spline cubique $C^2$ recherchée est dite "naturelle".
            
		\subsection{Splines cubiques $C^2$ de lissage}
		
		    Une spline de lissage permet de satisfaire un compromis entre la fidélité aux observations bruyantes et le lissage de la spline ajustée.
		
		    Les splines de lissage sont des splines cubiques dont chaque polynôme de degré trois est une approximation des données se trouvant sur son intervalle de définition. Cette spline ne passera pas (dans la plupart des cas)  par tous les points. Elle  minimise en revanche une quantité liée à la distance entre les données et la spline. En général, la spline approximant les données cherche à minimiser l'erreur au carré : c'est l'approximation aux moindres carrés.  Ces splines de lissage permettent d'éviter les oscillations qui seraient présentes avec une spline naturelle passant par tous les points, provoquées avec un nombre de données très grand. 
		    
		    Notre premier but, avant d'entrer dans le vif du projet, était de reprendre les programmes écrits pendant les cours d'algèbre linéaire pour le graphique et la CAO (Conception assistée par ordinateur). Ces programmes étant adaptés à des énoncés de travaux pratiques, nous les avons donc repris pour les compléter et les adapter à nos besoins.  De plus amples d'informations sur ces travaux se trouvent dans le polycopié de cours et sur les énoncés fournis par Mr Biard aux étudiants L3 MI disponibles depuis sa \href{http://www-ljk.imag.fr/membres/Luc.Biard}{page} internet personnelle.
	        

			\subsubsection{Répartition uniforme}
			
                Ce qui différencie les splines de lissage uniforme des autres splines de lissage, c'est la répartition des noeuds délimitant les différents polynômes cubiques : elle est uniforme. Cela signifie que les noeuds sont distribués de manière équidistante dans un intervalle déterminé, avec un pas strictement positif h = $x_{i+1} - x_i, i = 1,...,n-1$ pour les $n$ données $\{x_1, ... x_{n}\}$. La figure ci-dessous illustre la répartition uniforme de 15 noeuds (avec différentes splines de lissage, pour un unique jeu de données).
                \begin{figure}[H]
                    \centering
                    \includegraphics[width=10cm]{Images/SplinesUnif.png}
                    \caption{Exemple de plusieurs courbes de lissages uniformes avec différents paramètres de lissage $\rho$  }
                \end{figure}
                
                La spline est ensuite construite. Pour cela, il faut trouver la valeur des dérivées en chaque noeud. On applique ensuite le procédé d'interpolation de Hermite. Pour faire court, cette interpolation $C^2$ consiste en la construction d'un polynôme qui doit à la fois : 
                \begin{itemize}
                \item[•] Coïncider avec celles de la fonction déduite de l'échantillon aux points donnés
                \item[•] Avoir une dérivée qui coïncide avec celle de la fonction déduite de l'échantillon aux points donnés
                \end{itemize}
                Comme dans notre cas nous considérons des splines cubiques $C^2$, cette interpolation s'applique également pour la dérivée secondaire.
                Pour plus d'informations sur la construction des splines de lissage, il faut se référer au cours de \href{http://www-ljk.imag.fr/membres/Luc.Biard/L3MI_cours/Splines.pdf}{CAO}.
                
            
			\subsubsection{Répartition non-uniforme}
			
			    Lorsque nous souhaitons répartir les noeuds de manière non-uniforme, il faut calculer les matrices nécessaires. On s'inspire des calculs pour le cas uniforme effectués par M. Biard dans son cours de \href{http://www-ljk.imag.fr/membres/Luc.Biard/L3MI_cours/Splines.pdf}{CAO}. \\
			    
			    Soient $\{(u_k,z_k), k\in\mathbf{N}$ $|$ $u_i < u_j, \forall (i,j) \in \mathbf{R}^2, i < j\}$ nos données. Considérons les $n \in \mathbf{N}$ noeuds de lissage $\{x_k, k\in\mathbf{N},$ $k < n$ $|$ $x_i < x_j, \forall (i,j) \in \mathbf{R}^2, i < j\}$
                et l'espace $S[x_1,x_n]$ des splines naturelles associées à ces points. \\
                
                On considère le problème d'optimisation suivant, qui est la minimisation d'une quantité : $$Min_{s \in S[x_1,x_n]}E_{0,2}(s)$$
                \begin{center}
                    avec
                \end{center} $$E_{0,2}(s)=\sum_{k=1}^{N}(z_k-s(u_k))^2+\rho\int_{x_1}^{x_n}[s^{''}(t)]^2$$
                $\rho$ est le paramètre de lissage de la spline, comme dans le cas uniforme.\\
                
                De la même manière que dans le cas uniforme (et avec les mêmes notations), on détermine les matrice A, R, S, M, N, $H_{0,3}$ et $H_{1,2}$. Commençons par les matrices A et R : on cherche, comme dans le cas uniforme, la relation entre y et y' pour une spline cubique naturelle $C^2$, qui peut être écrite de la forme Ay' = Ry. Nous avons les conditions 
                $s''(x_1)=s''x_n=0$ (spline naturelle) et $s''_{i-1}(x_i)=s''_i(x_i)$ (contact aux noeuds $C^2$).\\
                Nous obtenons, en appliquant la même méthode que dans le cas uniforme, ces trois relations entre y et y' :\\
                \begin{cases}
                $2y'_1+y'_2 &=3/h_1(y_2-y_1)$ \\
                $h_iy'_{i-1}+2(h_{i-1}+h_i)y'_i+h_{i-1}y'_{i+1} &= 3(\frac{-h_i}{h_{i-1}}y_{i-1} + (\frac{h_i}{h_{i-1}} - \frac{h_{i-1}}{h_i})y_i + \frac{h_{i-1}}{h_i}y_{i+1}) &, i = 2...n-1$\\
                $y'_{n-1}+2y'_n &=3/h_{n-1}(y_n-y_{n-1})$
                \end{cases}\\
                La relation entre y et y' étant de la forme Ay'= Ry, on obtient \\
                $$A=\begin{pmatrix} 2&1&0&0&...&...&...&0&0 \\ h_2&2(h_1+h_2)&h_1&0&...&...&...&0&0 \\ \vdots&.&.&.&.&...&.&.&.\\ \vdots&.&.&.&h_i&2(h_{i-1}+h_i)&h_{i-1}&0&\vdots \\. \\.&.&.&.&.&...&.&.&. \\ 0&0&.&.&.&...&.&1&2 \end{pmatrix}$$\\
                
                $$R=3\begin{pmatrix} -\frac{1}{h_1}&\frac{1}{h_1}&0&0&...&...&...&0&0 \\ \frac{h_2}{h_1}&\frac{h_2}{h_1}-\frac{h_1}{h_2}&\frac{h_1}{h_2}&0&...&...&...&0&0 \\ \vdots&.&.&.&.&...&.&.&.\\ 0&.&.&.&\frac{-h_i}{h_{i-1}}&\frac{h_i}{h_{i-1}}-\frac{h_{i-1}}{h_i}&\frac{h_{i-1}}{h_i}&\vdots&0 \\.&.&.&.&.&...&.&.&. \\ 0&0&.&.&.&...&.&-1/h_{n-1}&1/h_{n-1} \end{pmatrix}$$\\
                
                Cherchons maintenant la matrice S. Pour cela, considérons l'intégrale suivante : \\
                $$\int_{x_1}^{x_n}[s''(t)]^2dt=\sum_{i=1}^{n-1}\int_{x_i}^{x_{i+1}}[s''(t)]^2dt$$
                En développant et en remplaçant le $h$ (du cas uniforme) par $h_i$ dans cette formule, on obtient une sorte de forme quadratique : $$\int_{x1}^{xn}[s''(t)]^2dt= Y''^T S y"$$
                Cela nous donne alors la matrice S. 
                $$S=1/3\begin{pmatrix} 2h_1&\frac{1}{2}h_1&0&0&...&...&...&0&0 \\ \frac{1}{2}h_2&2h_2&\frac{1}{2}h_2&0&...&...&...&0&0 \\ \vdots&.&.&.&.&...&.&.&.\\ \vdots&.&.&.&\frac{1}{2}h_i&2h_i&\frac{1}{2}h_i&0&\vdots \\. \\.&.&.&.&.&...&.&.&. \\ 0&0&.&.&.&...&.&\frac{1}{2}h_{n-1}&2h_{n-1} \end{pmatrix}$$\\
                
                Cherchons maintenant les matrices M et N. Pour les obtenir, nous exprimons le vecteur y" en fonction des  vecteurs y et y'. Soit s \in S[$x_1,x_n$] la spline naturelle correspondant à nos données. Soient $s_i \in \textbf{R}^3,i=1,..,n-1$ les polynômes cubiques définissant la spline, avec $s_i$ le polynôme cubique défini entre $x_i$ et $x_{i+1}$. On applique alors l'interpolation d'Hermite à ces polynômes cubiques avec la condition suivante :
                
                $$y"_i=s"_{i-1}(x_i)=s"_i(x_i), i=2,..n-1$$
                qui mène à $$y"=My+Ny'$$ Donc :
                $$M=3\begin{pmatrix} \frac{1}{h_1^2}&-\frac{1}{h_1^2}-\frac{1}{h_2^2}&\frac{1}{h_2^2}&0&0&...&...&...&0&0 \\ \vdots&...&\vdots&.&...&...&...&.&\vdots \\ \vdots&.&.&.&.&...&.&.&.\\ \vdots&.&.&.&\frac{1}{h_{i-1}^2}&-\frac{1}{h_{i-1}^2}-\frac{1}{h_i^2}&\frac{1}{h_i^2}&0&\vdots \\. \\.&.&.&.&.&...&.&.&. \\ 0&0&.&.&.&...&\frac{1}{h_{n-2}^2}&-\frac{1}{h_{n-2}^2}-\frac{1}{h_{n-1}^2}&\frac{1}{h_{n-1}^2} \end{pmatrix}$$
                \\
                \\$$N=\begin{pmatrix} \frac{1}{h_1}&\frac{2}{h_1}-\frac{2}{h_2}&-\frac{1}{h_2}&0&0&...&...&...&0&0 \\ \vdots&...&\vdots&.&...&...&...&.&\vdots \\ \vdots&.&.&.&.&...&.&.&.\\ \vdots&.&.&.&\frac{1}{h_{i-1}}&\frac{2}{h_{i-1}}-\frac{2}{h_i}&-\frac{1}{h_i}&0&\vdots \\. \\.&.&.&.&.&...&.&.&. \\ 0&0&.&.&.&...&\frac{1}{h_{n-2}}&\frac{2}{h_{n-2}}-\frac{2}{h_{n-1}}&-\frac{1}{h_{n-1}} \end{pmatrix}$$\\
                
                En appliquant la même méthode que dans le cas uniforme, on construit les matrices $H_{03}$ et $H_{12}$. Les notations utilisées sont celles du cours de CAO, identiques à celles du cas uniforme.
                $$H_{03}=\begin{pmatrix} H_0(t_1^1)&H_3(t_1^1)& & & & & & & &  \\ \vdots&\vdots& & & & & & &  \\ H_0(t_{k_1}^1)&H_3(t_k1^1)& & & & & & & \\ &H_0(t_{{k_1}+1}^2)&H_3(t_{{k_1}+1}^2)& & & & & &   \\ &\vdots&\vdots& & & & & &  \\ &H_0(t_{k_2}^2)&H_3(t_{k_2}^2)& & & & & & \\ &\ddots&\ddots & & & & & & & \\ & & & & & & &H_0(t_{{k_{n-2}}+1}^{n-1}) &H_3(t_{{k_{n-2}}+1}^{n-1})\\ & & & & & & &\vdots &\vdots\\ & & & & & & &H_0(t_N^{n-1}) &H_3(t_N^{n-1}) \end{pmatrix}$$
                
                
                $$H_{12}=\begin{pmatrix} h_1H_1(t_1^1)&h_1H_2(t_1^1)& & & & & & & &  \\ \vdots&\vdots& & & & & & &  \\ h_1H_1(t_{k_1}^1)&h_1H_2(t_k1^1)& & & & & & & \\ &h_2H_1(t_{{k_1}+1}^2)&h_2H_2(t_{{k_1}+1}^2)& & & & & &   \\ &\vdots&\vdots& & & & & &  \\ &h_2H_1(t_{k_2}^2)&h_2H_2(t_{k_2}^2)& & & & & & \\ &\ddots&\ddots & & & & & & & \\ & & & & & & &h_{n-1}H_1(t_{{k_{n-2}}+1}^{n-1}) &h_{n-1}H_2(t_{{k_{n-2}}+1}^{n-1})\\ & & & & & & &\vdots &\vdots\\ & & & & & & &h_{n-1}H_1(t_N^{n-1}) &h_{n-1}H_2(t_N^{n-1}) \end{pmatrix}$$
                


    			Les tests ont été réalisés sur des cas précis et sur un jeu de cas aléatoires.
    
                \begin{itemize}
                \item[•] Cas d'une distribution uniforme : on trouve le même résultat qu'avec les matrices du cas uniforme. (Figure \ref{NUL1})
                \item[•] Cas d'une distribution de Chebichev (explications dans le cours de CAO) (Figure \ref{NUL2})
                \item[•] Cas aléatoires : On remarque comme attendu que si la distribution est trop chaotique, alors l'interpolation ne sera plus forcément optimisée. Cela dépend beaucoup des données. (Figures \ref{NUL3},\ref{NUL4})
                \end{itemize}
                
                \begin{figure}[H]
                \begin{center}
                \includegraphics[width=8cm]{Images/NUL_unif} 
                \end{center}
                \caption{Exemple sur une répartition uniforme}
                \label{NUL1}
                \end{figure}
                
                \begin{figure}[H]
                \begin{center}
                \includegraphics[width=8cm]{Images/NUL_Chebychev} 
                \end{center}
                \caption{Exemple sur une répartition de Chebichev}
                \label{NUL2}
                \end{figure}
                
                \begin{figure}[H]
                \begin{center}
                \includegraphics[width=8cm]{Images/NUL_alea} 
                \end{center}
                \caption{Exemple sur une répartition aléatoire (1)}
                \label{NUL3}
                \end{figure}
                
                
                \begin{figure}[H]
                \begin{center}
                \includegraphics[width=8cm]{Images/NUL_alea2} 
                \end{center}
                \caption{Exemple sur une répartition aléatoire (2)}
                \label{NUL4}
                \end{figure}
                

			\subsubsection{Choix du paramètre de lissage}
                Comme vu précédemment, il existe un paramètre de lissage $\rho$. Ce paramètre peut être vu comme un compromis entre la fidélité de la courbe aux données et la robustesse de l'interpolation. Un de nos objectifs porte sur le calcul automatique de ce paramètre par l'intermédiaire de notre application écrite en python.\\
                
                Après de nombreuses recherches sur une telle optimisation, nous avons conservé la méthode d'optimisation utilisée sur la tendance linéaire locale de Holt, appelée lissage exponentiel. 
                Les méthodes de lissage exponentiel consistent en des moyennes pondérées des observations passée. Ce sont des méthodes de proximité, les poids se dégradant de façon exponentielle à mesure que les observations vieillissent. Autrement dit, plus l’observation est récente, plus le poids associé est élevé. 
                Dans le cas de notre projet, le lissage étudié est le lissage exponentiel simple (SES, simple exponential smoothing). Cela signifie que l'on cherche à obtenir une valeur lissée en t pour la reporter tout simplement en t + 1.
                
                Ce  paramètre de lissage peut évidemment être choisi de manière subjective : par exemple en précisant sa valeur en fonction de l'expérience précédente. Cependant, une façon plus fiable et objective d’obtenir une valeur pour ce paramètre inconnu est de l'estimer à partir des données de l'échantillon. La méthode pour trouver ce paramètre est similaire à celle d'une estimation des coefficients d’un modèle de régression :  il faut minimiser la somme des résidus au carré. Cette minimisation est habituellement appelée «moindres carrés ». Soient $e_t=y_t - \hat{y}_{t|t-1}$ les résidus, pour $t=1,\dots,T$. Nous voulons trouver la valeur de $\rho$ qui minimise la somme des carrés de ces résidus :\\
                $$\text{erreur} =\sum_{t=1}^T (y_t - \hat{y}_{t|t-1})^2
                $$
                Cependant, contrairement au cas de régression, cela implique un problème de minimisation des moindres carrés non linéaire.
                Après plusieurs tentatives vaines d'implémenter une méthode répondant à ce besoin, nous utiliserons seulement  l'outil d’optimisation lié à la méthode de Holt, implémenté sous python. Ce choix permet de proposer a l'utilisateur un paramètre de lissage basé sur des fondements mathématiques, avant de le laisser décider s'il conserve ce paramètre ou s'il en choisit un autre.\\
                
                Notre méthode recherchant la minimisation des moindres carrés pondérés non linéaire, il est par conséquent difficile d'avoir un bon jugement sur le fonctionnement de notre méthode. On peut constater des exemples sur lesquels le choix du paramètre du lissage semble logique :
                
                \begin{figure}[!htb]
                \minipage{0.32\textwidth}
                    \includegraphics[width=\linewidth]{Images/ExempleFonctionnel .png}
                     \caption{Un premier exemple}\label{fig:Exemplefonctionnel}
                \endminipage\hfill
                    \minipage{0.32\textwidth}
                    \includegraphics[width=\linewidth]{Images/ExempleFonctionnel 2.png}
                     \caption{Un deuxième exemple où $\rho = 0 $}\label{fig:Exemplefonctionnel2}
                \endminipage\hfill
                \minipage{0.32\textwidth}%
                    \includegraphics[width=\linewidth]{Images/ExempleFonctionnel 3.png}
                    \caption{Un troisième exemple où $\rho = 1 $}\label{fig:Exemplefonctionnel3}
                \endminipage
                \end{figure}
                
                Autrement nous pouvons avoir des exemples où nous pouvons douter de la bonne valeur choisie par l'estimation automatique de $\rho$ :
                
                \begin{figure}[H]
                \begin{center}
                \includegraphics[width=8cm]{Images/Exemple non fonctionnel.png} 
                \end{center}
                \caption{Exemple d'un cas à priori non fonctionnel}
                \label{CasNonfonctionnel}
                \end{figure} 
                
                Plusieurs hypothèses peuvent en découler : Utilise-t-on mal l'implémentation python ? Est-ce nos splines crées à partir de notre cours de CAO, dans lequel le paramètre de lissage n'est pas compris entre 0 et 1, qui posent problème?
                
                Malgré tout, nous savons donc maintenant créer une spline de lissage de manière totalement automatique, mais celle-ci tient compte de toutes les données sans distinctions. La suite de ce compte rendu va illustrer les techniques d'identification des valeurs aberrantes, d'attribution de poids faibles, et de suppression, et va présenter des algorithmes d'interpolation robustes.


	\section{Données aberrantes}

		\subsection{Les méthodes étudiées}
		   	Il existe plusieurs méthodes différentes pour interpoler des données malgré des points aberrants. La méthode la plus intuitive est de tracer la spline, retirer les points trop éloignés de celle-ci et recommencer, jusqu'à ce que tous les points soient proches de la spline construite. En faisant des recherches, nous avons découvert deux autres approches : une approche en trois étapes (détection, traitement, construction de la spline), et une approche robuste. Différentes possibilités vont être évoquées dans ce rapport pour chacune de ces approches, après la partie décrivant la méthode intuitive.
            
			\subsubsection{Méthode intuitive}
			
			   
                Dans un premier temps, nous avons créé un algorithme qui, à partir d'un point et d'une courbe, renvoie la distance (euclidienne) du point à la courbe.
                
                Ensuite, nous avons écrit et implémenté l'algorithme de comparaison de l'erreur au seuil d'erreur, donné en paramètre, qui permet de déterminer les points aberrants dans les données fournies, par rapport à une spline donnée.
                
                Cela nous a alors permis de créer l'algorithme complet qui calcule une spline, trouve le point le plus éloigné de la courbe et le supprime des données si sa distance à la courbe est supérieure au seuil (ce qui veut dire qu'il est aberrant). On répète ces opérations jusqu'à ce que le point le plus éloigné ne soit plus détecté comme aberrant. La spline est recalculée après chaque suppression de point, car un point aberrant attirait la courbe vers lui inutilement.\\
                
                 Le seuil d'erreur à partir duquel une donnée est décrétée aberrante est très important : il doit être choisi avec soin en fonction des données, et conditionne la précision (et même la réussite) de cette méthode. Plus ce seuil est petit, plus le nombre de points considérés comme aberrants augmente, ce qui peut poser des problèmes de complexité, ou encore engendrer un résultat faux en éliminant des données nécessaires, qui ne sont pas aberrantes.
                
                
			\subsubsection{Traitement avant la création de la spline}
			    Voici les étapes des algorithmes qui traitent les points aberrants avant de créer la spline :
			    \begin{enumerate}
                    \item Détection des points aberrants
                    \item Traitement des points aberrants
                    \item Calcul et représentation graphique de la spline
			    \end{enumerate}
			    
				\paragraph{Winsorization}
				    La Winsorization est une manière de traiter les points extrêmes, qui sont en général les points aberrants. Cette méthode effectue les étapes 1 et 2 de la création de la spline avec traitement préalable, ce qui veut dire qu'elle détecte et traite les points aberrants, mais ne construit pas la spline. Il suffit de construire la spline de lissage associée aux données modifiées par cette méthode, pour avoir l'interpolation des données malgré les points aberrants.\\
				    
				    Cet algorithme est plutôt intuitif : il récupère le pourcentage de valeurs données comme aberrantes et modifie les valeurs extrêmes. La modification est équitablement répartie entre les valeurs trop grandes et celles trop petites. La valeur affectée à ces points est la plus proche (la plus grande pour les valeurs trop grandes, la plus petite pour celles trop petites) qui n'est pas aberrante.\\
				    
                    Cet algorithme fonctionne également si le pourcentage donné est (un peu) trop grand. En effet, si les valeurs ne sont pas aberrantes, elles vont être proches des autres, et leur valeur ne sera donc que très peu modifiée.

				\paragraph{Gestion des intervalles locaux}  
    				Un problème s'est posé alors que l'on n'y avait pas pensé durant la planification des tâches : beaucoup de méthodes de détection supposent que la répartition des points suit une loi normale. Cela veut dire que les points doivent être proches (en ordonnées) les uns des autres. Pour cela, on doit découper les données à interpoler en plusieurs intervalles qui contiennent des points ayant presque la même valeurs (et éventuellement quelques points aberrants).
    				
    				En effet, si par exemple on prend la fonction identité discrétisée sur [0,10] avec un point aberrant (1,10), ce point ne sera pas détecté si l'on considère tous les points en même temps, car si celui-ci est décrété aberrant (car il se situe trop loin de la moyenne par exemple), alors (10,10) sera aussi considéré comme aberrant bien que ce ne soit pas le cas. 
    				
    				\begin{figure}[H]
                \begin{center}
                \includegraphics[width=8cm]{Images/illustration_exemple_local} 
                \end{center}
                \caption{Illustration de l'exemple évoqué}
                \label{NUL3}
                \end{figure}
    				
    				Il faut donc trouver un moyen de séparer les points en groupes de points pas trop éloignés (excepté éventuellement les aberrants) afin de les étudier groupe par groupe.
    				
    				
				   Pour adapter une série de données en plusieurs petites séries qui semblent suivre une loi normale, nous avons développé deux méthodes. La première, intuitive, ne nous donnait pas satisfaction. C'est pourquoi nous avons cherché à penser d'une manière différente pour développer une seconde méthode, qui n'a rien donc rien à voir avec la première.
				
				    \subparagraph*{Méthode par pas}
				    
    				    Au départ nous avons considéré nos données (x,y) comme des points qui proviennent d'une fonction. Nous avons donc naturellement pensé à créer des intervalles en étudiant la variation de y en fonction de x.\\
    				    
    				    Soit $n$ le nombre de données.
                        Soient $dy_i=|y_{i+1}-y_i|$  et $\Delta y_j=|dy_{j+1}-dy_j|$,  avec  $0\le i < n$, $ 0 \le j< n-1$. Cette méthode fonctionne comme suit : on prend un intervalle $[y_d,…,y_f ]$ ($d,f \in \mathbf{N}, 0 \le d \le f < n$, $d = 0$ et $f = 1$ lors de l'appel initial) et on regarde si $\Delta y_{f-1}<\epsilon$.
                        \begin{itemize}
                        \item 	Si $\Delta y_{f-1}<\epsilon$ on rajoute le point suivant à l'intervalle et on teste $\Delta y_f$, puis on continue récursivement
                        \item 	Sinon on ferme l'intervalle courant et on en commence un nouveau qui va contenir initialement deux éléments$[y_f,y_{f+1}]$ (sauf si on est à la fin de l'intervalle).
                        \end{itemize}
                        Pour trouver le $\epsilon$ qui convient à chaque série des données, on a développé une fonction qui estime ce paramètre automatiquement. On commence par estimer le paramètre en fonction des données, puis on utilise la fonction de création d'intervalles à partir du $\epsilon$ trouvé, passé en argument.
                        
				    \subparagraph*{Méthode par densité}
    				    Cette méthode, contrairement à la première, considère les points comme des boules placées sur une barre. On a donc pensé à déterminer nos intervalles en fonction de la densité de ces boules sur la barre, c'est à dire en fonction de la densité des abscisses des points sur chaque intervalle.\\
    				    
                        Pour commencer nous avons considéré notre problème comme un problème de maximisation, c'est à dire comme la recherche d'une méthode qui nous permet de trouver des intervalles dont la densité est maximale.
                        
                        
                        Pour trouver un résultat optimal, nous avons essayé de modéliser le problème mathématiquement :\\
                        soient (x,y) un ensemble de points et ds(d,f) la densité de l'ensemble des points
                        $\{( x_d, y_d),…,( x_f, y_f ) \}$sur l'intervalle $[ x_d,x_f]$
                        	
                        \begin{flushleft}
                            \bf
                            
                        
                        $$S(d,f)= max (ds(d,f),max_{j=d+1,...,f-1}(ds(d,j)+ds(j,f)))$$
                         \end{flushleft}
                        S(d, f) est la somme des densités maximales de tous les intervalles .
                      A partir de cette formule, nous avons développé une méthode qui renvoie les intervalles de densité maximale.  \\
                        Cette méthode est basée sur trois fonctions :
                        \begin{itemize}
                        \item densite\textbf{(x,d,f)} : cette fonction renvoie la densité de l'ensemble des points $\{( x_d, y_d),…, ( x_f, y_f ) \}$ sur l’intervalle $[ x_d,x_f  ]$
                        \item ind-int\textbf{(x,d)}:cette fonction renvoie l'indice fin de l'intervalle de densité maximale qui commence par l'élément d'indice d . 
                        \item 	ind-densite\textbf{(x)}: cette fonction renvoie une liste de tous les intervalles de densité maximale
                        \end{itemize}
				
				    \subparagraph*{Regroupement d'intervalles}
				        Les intervalles obtenus avec les méthodes précédentes donnent des fois de petits intervalles, sur lesquels certaines méthodes fonctionnent moins bien (en particulier les méthodes interquartile et la méthode des k voisins, qui nécessitent un nombre minimum de données pour renvoyer un résultat correct). Nous avons donc implémenté une fonction qui regroupe tous les intervalles de taille inférieure à un paramètre avec leurs voisins jusqu'à ce que la taille de tous les intervalles soit supérieure ou égale à ce paramètre. 
				        
				    
				
				

				\paragraph{Méthodes de détection des points aberrants}
    			    Dans cette partie, nous allons présenter plusieurs méthodes de détection de points aberrants. Elles nécessitent, pour certaines, des paramètres. Cela peut être un coefficient, un taux d'erreur... Ceux-ci sont à adapter manuellement à chaque exemple. Les estimer automatiquement serait possible, mais nous ne l'avons pas fait, nous en parlerons dans les tâches non réalisées(METTRE ICI LE BON TITRE DU 1 DE LA CONCLUSION), dans la conclusion du rapport.
                    
					\subparagraph*{Méthode inter-quartiles}
                        					
                       Cette méthode, comme son nom l'indique, détecte les points aberrants en utilisant le 1er et le 3e quartile. Cette méthode est aussi nommée méthode de la boîte à moustaches, ou encore méthode de la boîte de Tukey. \\
                       
                       L'algorithme de cette méthode est implémenté en deux fonctions. La première fonction calcule dans un premier temps les premiers et troisièmes quartiles afin de récupérer l'écart inter-quartiles. Ensuite, elle construit et renvoie un intervalle de confiance , défini de la manière suivante, avec $Q1$ et $Q3$ les premier et troisième quartiles, $écart\_interquartile$ la distance entre $Q1$ et $Q3$ ($écart\_interquartile = Q3 - Q1$), et $coeff$ un certain coefficient valant en général $1.5$ : $[Q1 - coeff*(Q3 - Q1), Q3 +coeff*(Q3 - Q1 )]$.
                       
                       Il faut fixer coeff correctement : s'il est trop grand, on ne détectera pas les points aberrants tandis que s'il est trop petit, des points seront déclarés aberrants alors qu'ils ne le sont pas.
                       
                       La seconde fonction renvoie vrai si et seulement si le point est aberrant, c'est à dire en dehors de l'intervalle calculé par la fonction précédente (cet intervalle est passé en paramètre).
                       
                    \subparagraph*{Test Tau de Thompson}
                    Le test de Thompson est un test statistique qui détecte les valeurs aberrantes parmi une série de valeurs. Ce test est considéré comme un des meilleurs critères de détection des valeurs aberrantes parce qu'il les détecte statistiquement en tenant compte de l'écart type et de la moyenne.\\
                    
                    
                    Pour ce test on définit deux quantités : $\delta_x=\frac{x-\bar{x}}{s}$ et seuil $=\frac{t_{\alpha/2}*(n-1)}{\sqrt{n}*\sqrt{n-2+t^2_{\alpha/2}}}$
                    \begin{itemize}
                    \item $\bar{x}$ est la moyenne de la série de valeurs
                    \item 	s est l'écart type de la série de valeurs
                    \item 	x est la valeur testé e
                    \item $\alpha$ est un paramètre que l'on fixe
                    \item $t_{\alpha/2}$ est la valeur critique provenant de la loi de Student avec une probabilité égale à  $\alpha/2$ et n-1 degré de liberté 
                    \item n est le nombre de valeurs 
                    \end{itemize}
                    Une valeur x est dite aberrante uniquement si $\delta_x$ est supérieur au seuil ou inférieur à l'opposé du seuil (-seuil).

					\subparagraph*{Test de Chauvenet}
                    					
                    Cette méthode permet de détecter les données aberrantes parmi une série de données. Ce test  considère que les données étudiées suivent une loi normale de moyenne égale à la moyenne empirique et de variance égale à la variance empirique.\\
                    
                    Le principe de cette méthode est simple : pour chaque donnée testée $x_t$, on calcule la probabilité d'avoir une donnée qui s'écarte de plus de $|x_t-\bar{x}|$ de la moyenne et on la multiplie par la taille de l'échantillon n.\\
                    	$N = n*P( |X-\bar{x}|>|x_t-\bar{x}|)$ avec $\bar{x}$ la moyenne empirique\\
                    	Si N est inférieur à tau, on considère que la donnée testée est aberrante sinon on admet qu'elle ne l'est pas. Tau est un paramètre de la méthode. 
                    					   
					\subparagraph*{Test de Grubbs}
                        					
                        Cette méthode a été inventée par Frank E. Grubbs en 1969. Pour ce test, on n’étudie que la valeur extrême (celle dont la valeur absolue de l’écart à la moyenne est la plus grande). Si plusieurs existent, on peut itérer ce test plusieurs fois tant que l'on trouve des points aberrants, en retirant le point aberrant trouvé. \\
                        
                        L'algorithme fonctionne comme suit : la valeur extrême est récupérée, puis l'on compare ensuite $\frac{v_{extrême}-moyenne}{écart\_type}$ avec le seuil critique donné par le test de Grubbs : $G_{crit} = \frac{n-1}{\sqrt{n}}\sqrt{\frac{t^2_{\frac{\alpha}{n},n-2}}{n-2+t^2_{\frac{\alpha}{n},n-2}}}$, avec $n$ le nombre de données, $t_{a,b}$ le résultat de la fonction quantile de Student avec un seuil de conﬁance $a$ et $b$ degrés de liberté, et $\alpha$ l’erreur que l’on accepte. Si la première valeur est plus grande que le seuil, alors la méthode de Grubbs considère que le point extrême est aberrant.
                        
                        $\alpha$ est un paramètre que nous décidons de passer à la fonction. Plus celui-ci est faible, plus la chance que les points détectés comme aberrants le soient réellement (mais dans ce cas, peu sont détectés, certains points pourtant aberrants peuvent ne pas être détectés)


					\subparagraph*{Méthode de la déviation extrême de Student}
					
                        Cette méthode a été inventée par Bernard Rosner en 1983. Ce test est une généralisation du test de Grubbs. En anglais, ce test appelé "extreme studentized deviate" est abbrégé ESD. L’algorithme suit les étapes suivantes : \\
                        - Récupération des valeurs extrêmes (même déﬁnition que dans le paragraphe précédent)\\
                        - Comparaison des valeurs extrêmes normalisées avec le seuil critique, qui dépend du nombre de valeurs extrêmes déjà enlevées. Valeur normalisée : $\frac{v_{extreme}-moyenne}{ecart-type}$ . Seuil critique, avec i le nombre de valeurs déjà enlevées : 
                        $\frac{    (n-i-1)*t _{     \frac{1-\alpha}{2(n-i)},n-i-2     }       }  {        \sqrt{       (n-i)*(n-i-2+t^2_{   \frac{1-\alpha}{2(n-i)},n-i-2} )      }     }  $ \\
                         - On ne compare pas les valeurs suivantes si un point est décrété comme non aberrant : en eﬀet, on traite les valeurs dans l’ordre de leur "extrêmitude".

				
					\subparagraph*{Méthode des k plus proches voisins}
					
					    Cette méthode est aussi appelée KNN pu k-NN, pour k nearest neighbors.
    					Cette méthode porte bien son nom : chaque point est comparé à ses k plus proches voisins afin de savoir s'il est aberrant ou non.\\
    					
    					
                        Cette méthode en abrégé k-NN ou KNN, consiste à, étant donné un ensemble A de n points, dans un espace métrique E, un entier K plus petit que n, et un point supplémentaire x( x dans ce cas est un point de A), trouver les K points de A les plus proches de x.
                        \newline
                        Calculer pour chaque observation (point) sa distance au k plus proche voisin, c'est à dire le kième élément avec la plus petite distance par rapport au point étudié pour k variant entre 1 et K (K inférieur à la taille des données) ; ordonner les observations suivant ces K-distances ; Les données aberrantes sont celles avec les plus grandes K-distances ; donc on trie les points suivant ces distances(ordre décroissant), les observations qui ont les n pourcent plus grandes distances K-distance sont des données aberrantes, n étant un paramètre à fixer.
                        \newline
                        \newline
                        Dans un premier temps, nous avons crée un algorithme qui prend en entrée une liste d’abs-
                        cisses et une autre d’ordonnées, un point abscisse et ordonnées et un entier k, retourne la
                        k-distance du point donné en entrée représentant la moyenne ses k petites distances.
                        \newline
                        Enfin, notre fonction KNN qui comme la précédente, prend 2 listes une d’abscisses et l’autre
                        d’ordonnées, un entier k et un entier m comme indiqué au dessus, retourne 4 listes les deux
                        premières(abscisses et ordonnées) contenant les valeurs de la liste qui ont les n pourcent plus
                        grande k-distance qui représentent ici les valeurs aberrantes et les 2 dernières les abscisses et
                        ordonnées des valeurs ici considérées comme non aberrantes.
                        
                        \subsection{limites de la méthode}
                        La recherche linéaire souffre d'un problème de lenteur. Si l'ensemble A est grand, il est alors extrêmement coûteux de tester les n points de l'espace.
                        
                        Les optimisations de cet algorithme se basent souvent sur des cas particuliers du problème. Le plus souvent, l'optimisation consiste à effectuer au préalable un algorithme (pré-traitement) pouvant avoir une complexité supérieure à O(n) mais permettant d'effectuer par la suite très rapidement un grand nombre de recherches. Il s'agit alors de trouver un juste milieu entre le temps de pré-calculs et le nombre de recherches qui auront lieu par la suite. 
                        
                        \subsection{Exemples d’exécutions}
                        Plus k est petit plus la méthode donne plus de précisions sur les valeurs aberrantes et donne un meilleur nuage de données.    					
    					la distance au K plus proche voisin k-distance; Ordonner les observations selon ces distances k-distance; Les données aberrantes ont les plus grandes distances k-distance; Les observations qui ont les n pourcent plus grandes distances k-distance sont des données aberrante, n étant un paramètre à ﬁxer. Dans un premier temps, nous avons crée un algorithme qui prend en entrée une liste et un indice, et retourne une liste contenant les distances de l’élément à la position i à aux éléments de la liste. Ensuite, un deuxième qui comme la précédente prend en entrée une liste, un indice et un entier k et retourne la k-distance de l’élément à la position indice qui représente la moyenne ses k petites distances. Enﬁn, un dernier qui elle prend une liste, un entier k et un entier n comme indiqué au dessus, retourne la liste contenant les valeurs de la liste qui ont les n pourcent plus grande k-distance.
    					

				\paragraph{Traitement des points détectés}
				
    				Dans cette partie, nous allons évoquer les différents traitement possibles des points aberrants avant l'interpolation des données.\\
                        
                    Deux algorithmes ont été implémentés pour le traitement des points (chacun pouvant être utilisé pour chaque méthode de traitement). AMELYS, EST CE QUE LES DEUX SONT ENCORE UTILISES OU JUSTE UN SEUL ? MODIFIER SI C'EST DEVENU FAUX !
                    Le premier algorithme prend en argument traite toutes les données en une fois, tandis que le deuxième ne traite que le point dont l'indice est passé en paramètre. 
                    
					\subparagraph*{Suppression}
                    					
                        La méthode la plus simple est de simplement supprimer les points aberrants des données utilisées pour le calcul de la spline. Cet algorithme est indépendant de la méthode de détection de points aberrants utilisée.
                        
					\subparagraph*{Méthode inspirée de la Winsorization}
					
    					La Winsorization, que nous avons déjà évoquée au paragraphe 2.1.2.1, nous a donné l'idée d'un autre traitement de valeurs aberrantes : les remplacer par la valeur non aberrante la plus proche. Ce n'est pas la Winsorization, car on ne supprime pas équitablement les valeurs trop grandes et celles trop petites.
    					
					\subparagraph*{Attribution de poids}
					    Chaque méthode de détection utilisée nous donne un vecteur de poids qui associe un poids faible pour les points détectés comme aberrants, soit inférieur à 1, et un poids fort égal à 1 pour ceux qui n'ont pas été détectés comme tels.
					    
					    Les nombreuses recherches effectuées nous ont permis de décider d'une valeur arbitraire pour ce poids faible, elle sera égale à $\frac{1}{taille \ de \ l'échantillon}$. Il nous reste à trouver comment utiliser ce vecteur de poids pour créer des splines de lissage moins impactées par les valeurs aberrantes. Une des idées était d'intégrer notre vecteur de poids dans une méthode faisant elle-même intervenir des poids. C'est là qu'intervient la méthode de régression linéaire pondérée LOESS, une méthode robuste qui réalise le traitement et l'interpolation de cette spline qui priorise les points non aberrants (expliquée de façon détaillée en 2.1.3).
				
					    
			    \paragraph{Construction de la spline}
			        En ce qui concerne la création de la spline, le système reste le même pour chaque méthode de détection utilisée, voyons cela avec la figure suivante :
			        
			        \begin{figure}[H]
                        \centering
                        \includegraphics[width=10cm]{Images/ConstructionDeLaSpline.png}
                        \caption{Exemple de construction}
                        \label{fig:exemple}
                    \end{figure}
                    
                    On peut voir que la spline est moins attirée par les données aberrantes en rouges puisqu'elles possèdent des poids plus faibles que celles considérés comme non aberrantes. Les données en vert sont les données estimées par LOESS avec un $\rho $ optimal de 0.05.  Ce sont ces nouvelles données, ces points estimés qui vont servir à créer une nouvelle spline, moins influencée par les valeurs détectées comme aberrantes par nos méthodes.
			        
			        EXPLIQUER ICI COMMENT EST CONSTRUITE LA SPLINE, EN QUELQUES MOTS EN FONCTION DES METHODES (Qui a fait ça ? Clément pour les poids ? Amélys pour le reste ?) Dire par exemple que plus le poids est faible, moins le point "attire" la spline (mais expliquer rapidement comment c'est fait), ... (Poids, points supprimés, points modifiés). Partie courte à priori.
					    
			\subsubsection{Interpolation robuste}
        		
        		Nous avons également trouvé lors de nos recherches des algorithmes robustes, c'est à dire des algorithmes dont le résultat n'est pas réellement influencé par les points aberrants. Ces méthodes calculent la spline tout en détectant les points aberrants au fur et à mesure.
        		
			    \paragraph{Algorithme de RANSAC}
    			   
                    RANSAC est l'abréviation de RANdom SAmple Consensus. Cet algorithme est non déterministe, cela veut dire qu'il peut renvoyer des résultats différents pour une même entrée (mêmes données et paramètres) à cause de l'utilisation du hasard.\\
                    Cet algorithme va calculer un bon nombre de splines et va choisir celle qui correspond aux données mais qui engendre le moins d'erreur possible entre la spline et les données décrétées comme non aberrantes. Voilà les étapes :\\
                    
                    \begin{enumerate}
                    \item
                    L'algorithme choisit un certain nombre de points (distincts) aléatoires
                    \item
                    Il calcule la spline cubique passant par ces points 
                    \item
                    Il mesure de la distance entre la courbe et chaque point afin de récupérer les points non aberrants d'après la spline créée
                    \item
                    Si les points non aberrants trouvés lors de l'étape précédente sont peu nombreux, au moins un point utilisé pour créer la spline a de grande chances d'être aberrant. L'algorithme passe donc à une nouvelle spline, en retournant à l'étape 1.
                    \item
                    Si la spline proche de beaucoup de points, c'est qu'à priori elle interpole la majorité des données. Les autres points sont décrétés comme aberrantes.  L'algorithme calcule la spline de lissage correspondant aux données considérées comme non aberrantes.
                    \item
                    La distance totale des points (non aberrants) par rapport à la courbe est calculée, c'est l'erreur de la spline de lissage étudiée. On sauvegarde cette spline si et seulement si elle est meilleur que la précédente (c'est à dire si cette erreur est plus faible), ou si c'est la première qui correspond aux données.
                    Une fois qu'un certain nombre de splines a été testé, c'est la dernière sauvegardée qui est considérée comme l'interpolation des données.
                    \end{enumerate} 
                    
                    Avec cette version de l'algorithme, beaucoup de paramètres sont nécessaires (en plus des données) et doivent être réglés manuellement, en fonction des données et de ce que l'on souhaite obtenir :
                    \begin{itemize}
                    \item[•] Le nombre de spline à créer
                    \item[•]  L'erreur acceptée entre les points et la spline cubique jusqu'à laquelle les points ne sont pas considérés comme aberrants à l'étape 3
                    \item[•]  La fonction de distance à utiliser aux étapes 3 et 6
                    \item[•] Le nombre de points non aberrants à partir duquel on considère que la spline correspond aux données
                    \item[•]  Le nombre de points à considérer lors de la construction de la spline de l'étape 2 (c'est à dire le nombre de points tirés à l'étape 1)
                    \item[•] Le paramètre de lissage de la spline à l'étape 5.
                    \end{itemize}
                    
                    Deux paramètres sont cependant estimables au fur et à mesure de l'algorithme :\\ le nombre de spline à créer (c'est à dire le nombre d'itérations), et le nombre de points minimum à considérer pour avoir un résultat correct. Cela n'est possible qu'à condition de fournir à la place la probabilité d'avoir un résultat correct (cette probabilité doit se trouver dans $]0,1[$.). \\
                    Les calculs, détaillés à la 6ième page du document cité en source BIEN METTRE LA SOURCE, nous donne le nombre d'itérations à effecteur pour avoir une probabilité $p_{correct}$ d'avoir un résultat correct, avec une proportion $\omega$ de données non aberrantes (aussi appelées inlier) : $n_{iter} = \frac{log(1 - p_{correct})}{log(1 - \omega^n)}$
                    
                    Les deux versions de l'algorithme évoquées ont été implémentées dans l'application, mais seule la seconde peut être lancée et appliquée sur les données.
                    Une première fonction implémente la première version évoquée, tandis qu'une seconde calcule au fur et à mesure la proportion de données non aberrantes de l'échantillon et actualise le nombre maximum d'itérations grâce au résultat évoqué précédemment.
                    C'est avec cette seconde que les tests et les comparaisons ont été effectués.
                    
                    Cet algorithme possède plusieurs limites. On va essayer d'en expliquer quelques-unes :
                    \begin{itemize}
                    \item[•] Il faut, pour l'instant, définir certains paramètres manuellement, en fonction des exemples.
                    \item[•] L'algorithme est non déterministe, ce qui signifie que le résultat peut, avec peu de chances, être totalement faux. De plus il varie en fonction des essais.
                    \item[•] Le paramètre de lissage est le même sur toute la fonction donc si celle-ci est stable à un endroit mais bouge beaucoup ailleurs, on ne peut pas avoir les deux. (test 22)
                    \item[•] Il est impossible (ou très difficile) de savoir si certains points sont aberrants ou pas (exemple num 20) : comment savoir si ce sont les deux/trois points du haut ou du bas qui sont aberrants ? (à gauche)
                    \item[•] Certains points sont décrétés comme aberrants alors que visuellement, ils ne le sont pas. (Exemple num 11)\\
                    \end{itemize}
                    ENLEVER LES NUMEROS DE TEST, MAIS METTRE UNE IMAGE COMME PREUVE
                   POSSIBLE QUE LORSQUE L'ESTIMATION DU PARAMETRE NE SERA PAS OBLIGATOIREMENT AUTOMATIQUE
                    
                    
			    \paragraph{Méthode de LOESS}
			    
                    LOESS (ou LOWESS) est une méthode robuste de régression non paramétrique. Elle utilise la régression linéaire des moindres carrés pondérés et créé une fonction qui décrit la variation des données, point par point, en considérant de manière plus importante les données les plus proches de ce point.
                    
                    L'objectif de cette méthode est d'ajuster $\theta = [\theta_0, \theta_1]$ pour minimiser les moindres carrés pondérés, c'est à dire la quantité suivante : \[\sum_{i=1}^m w_i ( y_i - (\theta_0 + \theta_1 x_i))^2 \ \ \text{(1)}\]
                    
                    Avec : 
                    \begin{itemize}
                        \item[•]  $\theta_0 \ et \ \theta_1$ les paramètres inconnus dont la valeur permet d'effectuer la procédure d'ajustement  
                        \item[•]  $(\theta_0 + \theta_1 x_i)$ la coordonnée en y prédite par la méthode en fonction de ces paramètres.
                        \item[•]  $1 > w_i = \exp \left( - \frac{(x -x_i)^2}{2 \rho} \right) > 0$ le poids Gaussien où  $\rho$ est considéré dans notre cas comme le paramètre de lissage.
                    \end{itemize}
                    
                    Les poids est donc donné en utilisant le calcul des moindres carrés, on a par conséquent :
                    \begin{itemize}
                        \item[•] Plus de poids à des points près du point cible $x$ 
                        \item[•] Moins de poids à des points plus loin de $x$
                    \end{itemize}
                    Autrement dit, si la différence $| x_i - x |$ est petite, alors le poids $w_i$ est proche de 1. Tandis que dans le cas contraire, si elle est grande, alors $w_i$ est proche de 0. 
                    Notre modèle est ajusté une fois la méthode appliquée, pour ne retenir que le point du modèle qui est proche du point cible. La procédure se répète pour chaque point par ordre croissant des abscisses.
                    
                    Partons de l'expression (1) avec  x et y  des vecteurs de taille m. Appelons cette expression S en fonction de $\theta$, nous avons :
                    \[S(\theta) = \sum_{i=1}^m w_i \left( y_i - (\theta_0 + \theta_1 x_i) \right)^2\]
                    
                    \[\frac{\partial S}{\partial \theta_0} = -2 \sum_{i=1}^m w_i \left( y_i - (\theta_0 + \theta_1 x_i) \right) \]
                    
                    \[ \frac{\partial S}{\partial \theta_1} = -2 \sum_{i=1}^m w_i \left( y_i - (\theta_0 + \theta_1 x_i) \right) x_i \]
                    
                    
                    Puis :
                    
                    \[\frac{\partial S}{\partial \theta_0} = \sum_{i=1}^m w_i \left( y_i - (\theta_0 + \theta_1 x_i) \right)  = 0\]
                    
                    \[ \iff \sum_{i=1}^m w_i  \theta_0 + \sum_{i=1}^m w_i  \theta_1 x_i  = \sum_{i=1}^m w_i y_i  \ \ \ \text{Eq. (1)}\]
                    
                    \[\frac{\partial S}{\partial \theta_1} = \sum_{i=1}^m w_i \left( y_i - (\theta_0 + \theta_1 x_i) \right) x_i  = 0\] 
                    
                    \[\iff \sum_{i=1}^m w_i  \theta_0 + \sum_{i=1}^m w_i  \theta_1 x_i x_i  = \sum_{i=1}^m w_i y_i  x_i \ \ \ \text{Eq. (2)}\]
                    
                    
                    En écrivant les équations  Eq. (1) et Eq. (2) sous forme de matrice $\mathbf{A \theta = b}$ nous obtenons :
                    
                    
                    
                        \[\begin{bmatrix} \sum w_i & \sum w_i x_i \\ \sum w_i x_i & \sum w_i x_i x_i \end{bmatrix}  \begin{bmatrix} \theta_0 \\ \theta_1 \end{bmatrix}   = \begin{bmatrix}  \sum w_i y_i \\  \sum w_i y_i x_i \end{bmatrix}\] 
                    
                        \[\iff \mathbf{A} \theta = \mathbf{b}\]
                    
                        \[\iff \theta = \mathbf{A}^{-1} \mathbf{b}\]
                    
                    Il nous suffit donc de résoudre cette matrice et de combiner notre vecteur de poids faible avec celui de cette méthode, soit :
                    \[1 > w_i = \exp \left( - \frac{(x -x_i)^2}{2 \rho} \right)*vpoids_i > 0\] 
                    où vpoids est notre vecteur de poids trouvé avec l'aide de nos méthodes de détection et affectant un poids faible aux points aberrants.
                    Ainsi, pour n'importe quel point étudié, les points aberrants ont un impact réduit sur le tracé de la spline.
                    
                    \begin{figure}[H]
                        \centering
                        \includegraphics[width=10cm]{Images/LOESS}
                        \caption{Exemple de résultat }
                        \label{fig:exemple}
                    \end{figure}
                    
                    Dans cet exemple, nous avons en rouge une spline de lissage et en bleu une autre spline de lissage utilisant la méthode LOESS.  Nous avons également les valeurs initiales de l'échantillon en rouge et celles ajustées en bleu. On remarque que la méthode LOESS considère moins les valeurs pouvant être considérées comme aberrantes ou bruitées.
                    
                    
                    On a donc une méthode fonctionnelle donnant un poids faibles aux points aberrants. Cependant, cette méthode peut devenir coûteuse en terme de temps lorsque l'échantillon devient grand.

	\section{Tests et comparaison des différentes méthodes}
        Après plusieurs tests de chaque méthode de détection des données aberrantes, nous avons constaté deux choses :
        \begin{itemize}
        \item Les paramètres de toutes les méthodes semblent dépendre de la taille de l’échantillon et non pas de la distribution des données 
        \item Ces paramètres semblent se stabiliser lorsqu’on augmente la taille de l’échantillon, autrement dit, ces paramètres convergent vers les valeurs fixées par défaut dans chaque fonction, lorsque la taille de l’échantillon augmente \\($\ge 100 $)
        \end{itemize}
        Voici ci-dessous le résultat d'une méthode particulière avec différentes tailles. Les données en rouges sont les données conservées tandis que celles en bleu sont considérées comme aberrantes par l'algorithme. La courbe bleue représente l'interpolation attendue.
        \begin{figure}[H] %on ouvre l'environnement figure
          \minipage{0.32\textwidth}%
        \includegraphics[width=\linewidth]{Images/chauv1.PNG} %ou image.png, .jpeg etc.
        \caption{\\Taille de l’échantillon = 30} %la légende
        \label{m15} %l'étiquette pour faire référence à cette image
        \endminipage
        \minipage{0.32\textwidth}%
        \includegraphics[width=\linewidth]{Images/chauv2.PNG}  
        \caption{\\Taille de l’échantillon = 100}
        \label{m5}
        \endminipage
        \minipage{0.32\textwidth}%
        \includegraphics[width=\linewidth]{Images/chauv3.PNG}  
        \caption{\\Taille de l’échantillon = 1000}
        \label{m35}
        \endminipage
        
        \end{figure}
        \\
        Pour tous les tests qui viennent, nous allons fixer la taille de l’échantillon à 200 pour les signaux stationnaires et à 300 pour les signaux non-stationnaires.
        
        
        \subsection{Tests des méthodes de création d'intervalles}
        Pour tester ces méthodes, nous avons simplement détecté les points aberrants. Pour reproduire ces tests, il faut choisir l'option ... AMELYS AJOUTER LA BONNE OPTION ICI. En effet, nous avons ici considéré seulement la détection, et donc ignoré les étapes de traitement de points aberrants et la construction de la spline.
        
        \subsubsection{Création d'intervalles selon un pas}
        Cette méthode, comme mentionné dans la partie la concernant, ne fonctionne pas dans tous les cas. Voici ci-dessous quelques cas avec lesquels elle fonctionne et d'autres avec lesquels elle ne fonctionne pas. Dans chaque exemple, la graine a été fixée afin de pouvoir reproduire le même test. La graine (seed) est celle fournie au générateur de signal : cela permet de "fixer" le hasard.
        
       \paragraph{Cas stationnaire (seed = 5505361)}\\
        	Dans le cas d’un signal variant peu, cette méthode est correcte avec certaines méthodes de gestion de données aberrantes (par exemple le test de Chauvenet), mais pas avec toutes (par exemple la déviation extrême de Student). METTRE ICI LES AUTRES METHODES (INDIQUER CELLES  QUI MARCHENT, CELLES QUI NE MARCHENT PAS) ZAK,BILEL
        	
        \begin{figure}[H]
            \centering
            \begin{subfigure}[b]{0.4\textwidth}
                \includegraphics[width=\textwidth]{Images/comp1.PNG}
                \caption*{\begin{center}Test de Chauvenet, \\ régularité = 0.9\end{center}}
            \end{subfigure}
            \begin{subfigure}[b]{0.4\textwidth}
                \includegraphics[width=\textwidth]{Images/comp2.PNG}
                \caption*{\begin{center}Déviation extrême de Student, \\régularité = 0.9\end{center}}
            \end{subfigure}
            \caption{}
        \end{figure}
        
        Cette méthode de création d'intervalles fonctionne mieux dans le cas de signaux avec de grandes variations comme l'illustrent les exemples suivants.
        
        \begin{figure}[H]
            \centering
            \begin{subfigure}[b]{0.4\textwidth}
                \includegraphics[width=\textwidth]{Images/kmn1.PNG}
                \caption*{\begin{center}k plus proches voisins, \\régularité = 0.65\end{center}}
            \end{subfigure}
            \begin{subfigure}[b]{0.4\textwidth}
                \includegraphics[width=\textwidth]{Images/kmn2.PNG}
                \caption*{\begin{center}Déviation extrême de Student, \\régularité = 0.7\end{center}}
            \end{subfigure}
            \caption{}
        \end{figure}
        
        
       
        \paragraph{Cas non-stationnaire (seed = 32427415 )}\\
        Les cas qui ne fonctionnent pas dans le cas stationnaire ne fonctionnent pas non plus dans le cas stationnaire, comme le montrent entre autres les figures suivantes.
        
        \begin{figure}[H]
            \centering
            \begin{subfigure}[b]{0.4\textwidth}
                \includegraphics[width=\textwidth]{Images/dev1.PNG}
                \caption*{\begin{center}Déviation extrême de Student,\\switch-prob = 0.1\end{center}}
            \end{subfigure}
            \begin{subfigure}[b]{0.4\textwidth}
                \includegraphics[width=\textwidth]{Images/grub1.PNG}
                \caption*{\begin{center}Test de Grubbs,\\ switch-prob = 0.2\end{center}}
            \end{subfigure}
            \caption{}
        \end{figure}
        
        Dans les cas où la méthode fonctionne sur un signal stationnaire, le résultat pour un signal non stationnaire est satisfaisant lorsqu'on prend de petites valeurs de switch-prob (paramètre lié à l'apparition d'une grande variation à chaque instant, donc lié à la largeur des créneaux) et reste acceptable pour de grandes valeurs, bien que moins précis.
        
        \begin{figure}[H]
            \centering
            \begin{subfigure}[b]{0.4\textwidth}
                \includegraphics[width=\textwidth]{Images/ch1.PNG}
                \caption*{\begin{center}Test de chauvenet, \\switch-prob = 0.1\end{center}}
            \end{subfigure}
            \begin{subfigure}[b]{0.4\textwidth}
                \includegraphics[width=\textwidth]{Images/ch2.PNG}
                \caption*{\begin{center}Test de chauvenet, \\switch-prob =0.5\end{center}}
            \end{subfigure}
            \caption{}
        \end{figure}
        
        
        \subsubsection{Création d’intervalles selon la densité}
        Cette méthode semble renvoyer un résultat correct dans tous les cas étudiés précédemment (stationnaire et non stationnaire, petites et grandes variations) et avec toutes les méthodes de détection de points aberrants étudiées.
        
        \begin{figure}[H] %on ouvre l'environnement figure
          \minipage{0.32\textwidth}%
        \includegraphics[width=\linewidth]{Images/ext1.PNG} %ou image.png, .jpeg etc.
        \caption{\\Déviation extrême de Student,\\ régularité = 0.9} %la légende
        \label{m15} %l'étiquette pour faire référence à cette image
        \endminipage
        \minipage{0.02\textwidth}
        \hfill
        \endminipage
        \minipage{0.32\textwidth}%
        \includegraphics[width=\linewidth]{Images/ext2.PNG}
        \caption{\\k plus proches voisins,\\ régularité = 0.65}
        \label{m5}
        \endminipage
        \minipage{0.02\textwidth}
        \hfill
        \endminipage
        \minipage{0.32\textwidth}%
        \includegraphics[width=\linewidth]{Images/ext3.PNG}  
        
        \caption{\\Déviation extrême de Student,\\ switch-prob = 0.1}
        
        \label{m35}
        \endminipage
        \end{figure}
        \\
        Cette méthode admet cependant la même limite que la précédente, dans le cas de signaux non stationnaires avec de grandes valeurs de switch-prob : elle fonctionne moins bien que ce que l'on pourrait attendre sur certains exemples. Voici ci-dessous deux exemples qui illustrent cette limite. On constate que les résultats obtenus restent pas très éloignés de ceux attendus : ils ne sont pas totalement faux, juste moins précis que ce qu'on pourrait attendre.
        \begin{figure}[H] %on ouvre l'environnement figure
          \minipage{0.4\textwidth}%
        \includegraphics[width=\linewidth]{Images/tch1.PNG} %ou image.png, .jpeg etc.
        \caption{\\Test de Chauvenet,\\switch-prob = 0.5,seed = 32427415} %la légende
        \label{m15} %l'étiquette pour faire référence à cette image
        \endminipage
        \minipage{0.4\textwidth}%
        \includegraphics[width=\linewidth]{Images/tch2.PNG}  
        \caption{\\Test de Chauvenet,\\switch-prob = 0.5, seed = 54441996}
        \label{m5}
        \endminipage
        \end{figure}
        
        \subsubsection{Fonction de regroupement}
        On a évoqué, lors de l'explication de l'utilité de cette fonction, que deux méthodes de détection de points aberrants nécessitent que les intervalles ne soient pas trop petits afin de fonctionner correctement : les méthodes interquartile et k plus proches voisins. L'exemple ci-dessous montre la détection obtenue avec l'une des deux méthodes, mais c'est similaire pour la seconde. Le paramètre t est le nombre minimal de points par intervalle.
         \begin{figure}[H] %on ouvre l'environnement figure
          \minipage{0.4\textwidth}%
        \includegraphics[width=\linewidth]{Images/int1.PNG} %ou image.png, .jpeg etc.
        \caption{inter-quartile, t=5} %la légende
        \label{m15} %l'étiquette pour faire référence à cette image
        \endminipage
        \minipage{0.4\textwidth}%
        \includegraphics[width=\linewidth]{Images/int2.PNG}  
        \caption{inter-quartile, t=10}
        \label{m5}
        \endminipage
        \end{figure}
        \\
        Après plusieurs autres tests, nous avons fixé le paramètre t de la fonction regroupe à 10 pour la méthode interquartile et à 30 pour la méthode des k plus proches voisins.
        
        
        \subsection{Comparaison des méthodes de détection des points aberrants}
        Les six méthodes de gestion de points aberrants donnent un résultat plus ou moins similaire, peu importe le signal choisi. On ne peut pas dire qu'une méthode fonctionne mieux que les autres parce qu'on peut toujours améliorer le résultat de chaque méthode en trouvant la valeur exacte de son paramètre pour un l'échantillon de points considéré. Voici les résultats très similaires des six méthodes de détection obtenus pour un exemple quelconque de signal stationnaire (seed = 5505361) :
        \begin{figure}[H] %on ouvre l'environnement figure
          \minipage{0.32\textwidth}%
        \includegraphics[width=\linewidth]{Images/fin1.PNG} %ou image.png, .jpeg etc.
        \caption{\\Test de Chauvenet} %la légende
        \label{m15} %l'étiquette pour faire référence à cette image
        \endminipage
        \minipage{0.02\textwidth}%
        \hfill
        \endminipage
        \minipage{0.32\textwidth}%
        \includegraphics[width=\linewidth]{Images/fin2.PNG}  
        \caption{\\Déviation extrême de Student}
        \label{m5}
        \endminipage
        \minipage{0.02\textwidth}%
        \hfill
        \endminipage
        \minipage{0.32\textwidth}%
        \includegraphics[width=\linewidth]{Images/fin3.PNG}  
        \caption{\\Test de Grubbs}
        \label{m35}
        \endminipage
        \end{figure}
        
        \begin{figure}[H] %on ouvre l'environnement figure
          \minipage{0.32\textwidth}%
        \includegraphics[width=\linewidth]{Images/fin4.PNG} %ou image.png, .jpeg etc.
        \caption{\\Inter-quartile} %la légende
        \label{m15} %l'étiquette pour faire référence à cette image
        \endminipage
        \minipage{0.02\textwidth}%
        \hfill
        \endminipage
        \minipage{0.32\textwidth}%
        \includegraphics[width=\linewidth]{Images/fin5.PNG}  
        \caption{\\k plus proches voisins}
        \label{m5}
        \endminipage
        \minipage{0.02\textwidth}%
        \hfill
        \endminipage
        \minipage{0.32\textwidth}%
        \includegraphics[width=\linewidth]{Images/fin6.PNG}  
        \caption{\\Test tau de Thompson}
        \label{m35}
        \endminipage
        \end{figure}
        
        A FAIRE ICI (ENCORE UN GROS BOUT DU RAPPORT) :
        RANSAC
        LOESS
        Méthode de détection intuitive (déjà faite par Mohamed, cf discord Béryl)
        
        COMPARER CES TROIS + LES SPLINES OBTENUES A PARTIR DE LA DETECTION (les 6) sur certains exemples !!!

\renewcommand\partname{}
\part{Produit rendu}

Expliquer que les méthodes de détection des points aberrants sont implémentées en deux parties : une partie qui fait les calculs pour tout un jeu de donnée, et une partie qui renvoie pour chaque point s'il est aberrant ou non (avec le résultat calculé précédemment en paramètre). Exemple, avec interquartile : pour un intervalle, une première fonction renvoie un intervalle, la seconde renvoie vrai ssi la donnée se trouve dans l'intervalle. 

\paragraph{tâche 0}
Le programme adapté possède trois principales fonctionnalités. Toutes s'appuient sur la lecture d'un fichier et créent une courbe (paramétrique ou non) interpolant les données.
Le choix de l'ordre des points est donné à l'utilisateur sous la forme de trois choix : 

\begin{itemize}
    \item[•] Traitement des données "telles quelles". Les données sont traitées dans l'ordre dans lesquelles elles ont été fournies, sous forme de courbe paramétrique.
    \item[•] Traitement des données selon la première coordonnée. Les données sont triées par ordre croissant selon la première coordonnée. Cela permet d'éviter d'avoir une courbe paramétrique qui reviendrait sur ses pas. La courbe obtenue est une fonction de $\mathbf{R}$ dans $\mathbf{R}$.
    \item[•] Traitement des données selon la deuxième coordonnée. Cela correspond à la même manipulation que précédemment, mais selon la deuxième coordonnée.
\end{itemize}


\renewcommand\partname{}
\part{Conclusion}
	\section{Bilan du travail réalisé}
    le projet initial a-t-il été suivi ?
    Comme souligné dans la sous-partie nommée Organisation en II-2, nous avons très rapidement dû modifier notre planification des tâches, nous avons rencontrés plusieurs difficultés durant ce stage applicatif :
    (ici on peut parler des difficultés rencontrées, les raisons de notre changement de planning / abandon des tâches / choix des solutions)
    
    (puis un changement de paragraphe pour conclure sur les objectifs qui n'ont donc pas été atteints, à cause des difficultés rencontrées)
    
    (puis un changement de paragraphe pour conclure sur les objectifs qui ont été atteints)
    
    (la section bilan du travail d'équipe est pour l'instant un prototype qui je  modifierai en fonction de ce qui sera mis dans cette partie)
    
	\section{Bilan du travail d'équipe}
	Concluons d'abord avec notre travail en équipe :
	Le confinement n'a pas été pratique, le télétravail à réduit cette sensation de convivialité que pourrait procurer un travail de groupe, et le partage d'écran pour faire comprendre son travail aux autres à montré ses limites. Mais nous avons tous oeuvré pour maintenir une bonne ambiance au sein du groupe. 
	L'application de communication Discord nous a permis d'avoir une bonne circulation de l'information, puisque nous avions principalement des traces écrites de nos discussions et de nos décisions. 
    Étant auteurs de notre planification des tâches et n'ayant pas ou peu d'expérience dans les projets de groupes sur plusieurs semaines, comme évoqué dans la partie II-2, nous sommes satisfait d'avoir garder en vue de livrer une application répondant à nos objectifs, au prix de certaines fonctionnalités. Cela est  principalement dû à la fluidité de nos interactions sur Discord, qui ont permis de communiquer sur nos avancées, et donc de réaligner nos priorités pour satisfaire nos objectifs et d'en répartir les responsabilités. 

	




\renewcommand\partname{}
\part{Bibliographie}
	%\url{http://w3.mi.parisdescartes.fr/~lomn/Cours/CV/SeqVideo/Material/RANSAC-tutorial.pdf} \\

Luc Biard, Page Internet Personnelle\\
\url{http://www-ljk.imag.fr/membres/Luc.Biard}\\
Luc Biard, Cours sur les splines de lissages\\
\url{http://www-ljk.imag.fr/membres/Luc.Biard/L3MI_cours/Splines.pdf}\\
Hyndman, Rob J., and George Athanasopoulos   Forecasting: principles and practice. OTexts, 2014.\\
\url{https://otexts.com/fpp2/ses.html}\\
Xavier Bourret Sicotte, Locally Weighted Linear Regression (Loess)\\
\url{https://xavierbourretsicotte.github.io/loess.html}\\
Wikipedia, Local Regression\\
\url{https://en.wikipedia.org/wiki/Local_regression}{Wikipedia, Local Regression}\\
Wikipedia, Méthode des moindres carrés\\
\url{https://fr.wikipedia.org/wiki/Méthode_des_moindres_carrés}\\


\url{https://fr.wikipedia.org/wiki/Donn\%C3\%A9e_aberrante} : beaucoup de méthodes \\
\url{https://lemakistatheux.wordpress.com/category/tests-statistique-indices-de-liaison-et-coefficients-de-correlation/ les-tests-pour-la-detection-doutliers/} Plusieurs méthodes expliquées permettant de détecter les outliers : Déviation extrême généralisée Studen (et autres méthodes \\
\url{ https://en.wikipedia.org/wiki/Grubbs\%27s_test_for_outliers} test de Grubbs \\
\url{https://ellistat.com/guide-dutilisateur/statistiques-descriptives/tests-de-valeurs-aberrantes/ test-de-grubb/} test de Grubbs (pratique seulement, pas de théorie)\\
\url{http://w3.mi.parisdescartes.fr/~lomn/Cours/CV/SeqVideo/Material/RANSAC-tutorial.pdf}\\
\url{https://blogs.sas.com/content/iml/2017/02/08/winsorization-good-bad-and-ugly.html}
\url{https://en.wikipedia.org/wiki/Winsorizing}

Pour les k plus proches voisins

https://medium.com/@kenzaharifi/bien-comprendre-lalgorithme-des-k-plus-proches-voisins-fonctionnement-et-impl%C3%A9mentation-sur-r-et-a66d2d372679

https://fr.qwe.wiki/wiki/K-nearest_neighbors_algorithm

http://exorciste2.free.fr/Amine/nouveau%20dossier/MOAB/coursDM_KNN.pdf

\renewcommand\partname{}
\part{Annexes}
	\section*{Annexe 1 : blablabla}

\begin{figure}
\begin{center}
%\includegraphics[width=8cm]{} 
\end{center}
%\caption{Exemple de traitements des données}
%\label{suppr}
\end{figure}





\end{document}
